{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### NEEDED EXECUTION FOR GOOGLE COLAB #####\n",
    "\n",
    "# #on importe notre google drive sur le serveur distant de Google \n",
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')\n",
    "\n",
    "# #penser à faire les importation de package qui ne sont pas nativement present sur le serveur \n",
    "# !pip install ray[tune]\n",
    "\n",
    "# #save des resultats : attention à ne pas les sauvegarder seulement sur le serveur Google, mais à les sauvegarder dans notre drive.\n",
    "# drivefolder = \"\"\n",
    "\n",
    "# #notre terminal pour se deplacer sur le serveur distant \n",
    "# %cd digit-recognizer\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "data_train, data_val = train_test_split(data ,test_size=0.1, random_state=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "###################################################################################\n",
    "'''\n",
    "Pour la prochaine fois : manière assez clair de transformer les rawdata, séparer x et y et tout convertir en torch dès le init. \n",
    "Et dans le len / getitem on met presque rien, sauf la fontion transform de pytorch\n",
    " '''\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data,transform=None, is_test=False):\n",
    "        super().__init__()\n",
    "        self.dataset = data\n",
    "        self.transform=transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset.iloc[index].to_numpy()\n",
    "        if self.is_test:\n",
    "            label = None                            #il faut dire ce qui est le Y et ce qui est le X pour chaque ligne \n",
    "            image = data.reshape(28, 28)\n",
    "        else:\n",
    "            label = data[0]                             #il faut dire ce qui est le Y et ce qui est le X pour chaque ligne \n",
    "            image = data[1:].reshape(28, 28)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image.astype(np.float32))\n",
    "        return image, label\n",
    "        \n",
    "transform = transforms.Compose([transforms.ToPILImage(),transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on crée une classe enfant de la classe Dataset de torch, qui permet de convertir en tenseur\n",
    "Tdata_train = MNISTDataset(data_train, transform= transform)\n",
    "Tdata_val = MNISTDataset(data_val,  transform= transform)\n",
    "Tdata_test = MNISTDataset(data_test, transform= transform, is_test=True)\n",
    "\n",
    "#important que le label soit un tenseur ou pas ? \n",
    "Tdata_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#affichage d'une image au hasard et du label associé\n",
    "import matplotlib.pyplot as plt\n",
    "i = np.random.randint(0, len(Tdata_train))\n",
    "plt.imshow(Tdata_train[i][0].reshape(28,28))\n",
    "plt.title(Tdata_train[i][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mise en forme de batches des données (DataLoader)\n",
    "torch.manual_seed(1)\n",
    "batch_size= 16\n",
    "DL_train = DataLoader(Tdata_train,batch_size,shuffle=True)\n",
    "DL_val = DataLoader(Tdata_val,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et sans utiliser le DataLoader ça donne quoi ? (à faire pour mieux comprendre)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ CNN model\n",
    "\n",
    "### a/ Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "#construction d'un CNN paramétrable \n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self,pooling = nn.MaxPool2d, conv = nn.Conv2d, kernel1 = 5, kernel2 = 5, nb_channel1 = 16, nb_channel2 = 16):\n",
    "        super().__init__()\n",
    "        self.image_size = 28\n",
    "\n",
    "        self.conv1 = conv(in_channels=1, out_channels= nb_channel1, kernel_size = kernel1)\n",
    "        self.image_size = self.image_size - (self.conv1.kernel_size[0] - 1)\n",
    "\n",
    "        self.pool1 = pooling(kernel_size = 3, stride = 1)\n",
    "        self.image_size = self.image_size - (self.pool1.kernel_size - 1)\n",
    "        \n",
    "        self.conv2 = conv(in_channels=16, out_channels= nb_channel2, kernel_size = kernel2)\n",
    "        self.image_size = self.image_size - (self.conv2.kernel_size[0] - 1)\n",
    "\n",
    "        self.pool2 = pooling(kernel_size = 3, stride = 1)\n",
    "        self.image_size = self.image_size - (self.pool2.kernel_size - 1)\n",
    "       \n",
    "        '''\n",
    "        comment faire pour recuperer la taille de l'output après un certain modèle ?\n",
    "        plutot que de calculer avec les formules ... \n",
    "        '''\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "\n",
    "        self.input_classifier = nb_channel2*self.image_size**2\n",
    "        self.lin1 = nn.Linear(self.input_classifier,120)\n",
    "        self.lin2 = nn.Linear(120, 10)\n",
    "        \n",
    "        self.features   = nn.Sequential(self.conv1,nn.ReLU(),self.pool1,self.conv2,nn.ReLU(),self.pool2)\n",
    "        self.classifier = nn.Sequential(self.lin1,nn.ReLU(),self.lin2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x.view(x.size(0), -1))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b/ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def accuracy(yhat,y):\n",
    "    # y encode les indexes, s'assurer de la bonne taille de tenseur\n",
    "    assert len(y.shape)==1 or y.size(1)==1\n",
    "    return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).double().mean()\n",
    "    \n",
    "\n",
    "\"\"\"TO DOOOOOO\"\"\"\n",
    "#paramétrer l'optimiseur, la loss, la taille de batch, \n",
    "# rajouter un stop si la loss en validation augmente...patience et early stopping\n",
    "def train(model,epochs,train_loader,test_loader, writer = None, device = 'cpu', lr = 1e-3): \n",
    "    optim = torch.optim.Adam(model.parameters(),lr = lr)    # choix optimizer\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss()                            # choix loss\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x,y in train_loader:                           # boucle sur les batchs  \n",
    "            optim.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)                # y doit être un tensor (pas un int)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)                             # attention, il peut y avoir un batch + petit (le dernier)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        if writer != None:\n",
    "            writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "            writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "            print(cumloss/count)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                if writer != None:\n",
    "                    writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                    writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
    "                print('                ' + str(cumloss/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model,Data_eval, device = 'cpu'):\n",
    "    true_pred = 0\n",
    "    tot_pred = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in Data_eval:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            tot_pred += 1\n",
    "            yhat = model(x).argmax(1)\n",
    "            if yhat == y:\n",
    "                true_pred += 1\n",
    "    return true_pred / tot_pred*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c/ Run and save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model,fichier): # pas de sauvegarde de l'optimiseur ici\n",
    "      \"\"\" sauvegarde du modèle dans fichier \"\"\"\n",
    "      state = {'model_state': model.state_dict()}\n",
    "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
    " \n",
    "def load_model(fichier,model):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "            state = torch.load(fichier)\n",
    "            model.load_state_dict(state['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hippo/Documents/digit-recognizer/HD_analysis.ipynb Cellule 15\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hippo/Documents/digit-recognizer/HD_analysis.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hippo/Documents/digit-recognizer/HD_analysis.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hippo/Documents/digit-recognizer/HD_analysis.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m MyCNN()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hippo/Documents/digit-recognizer/HD_analysis.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m1stConv-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mtime\u001b[39m.\u001b[39masctime() \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hippo/Documents/digit-recognizer/HD_analysis.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m'''attention il faut que le nom du fichier contienne toutes les infos sur les hyperparamètres utilisés dans le modèle, \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hippo/Documents/digit-recognizer/HD_analysis.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mpour pouvoir le recharger ensuite (ou bien sauvegardé en plus un fichier de la configuration du modèle utilisé'''\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyCNN' is not defined"
     ]
    }
   ],
   "source": [
    "### entraineemnt long des modèle en DL, il faut privilégier de charger un modèle (puis fine-tuning)\n",
    "import time \n",
    "\n",
    "device = 'cpu'\n",
    "model = MyCNN().to(device)\n",
    "model.name=\"1stConv-\"+time.asctime() \n",
    "'''attention il faut que le nom du fichier contienne toutes les infos sur les hyperparamètres utilisés dans le modèle, \n",
    "pour pouvoir le recharger ensuite (ou bien sauvegardé en plus un fichier de la configuration du modèle utilisé'''\n",
    "#writer = SummaryWriter(f\"runs/\")\n",
    "epochs = 20\n",
    "train(model, epochs, DL_train, DL_val)\n",
    "file = model.name\n",
    "save_model(model,'models/' + file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d/ Comparaison de modèles et optimisation\n",
    "\n",
    "* optimisation des pretraitements // features engineering\n",
    "* On effectue un GridSearch des hyperparamètres de notre modèle\n",
    "* On optimise l'accuracy avec un grand nombre d'initialisation de poids différents\n",
    "* Utilisation de méthodes d'ensemble\n",
    "* Evolution de l'architecture du modèle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas réussi à utiliser correctement raytune...\n",
    "______________________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE OF RAY TUNE for the gridsearch of hyperparameters\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on a besoin de redefinir une fonction de train qui va etre appelée par tune.run\n",
    "# la fonction doit incorporer l'initialisation du modèle avec les hyperparamètres données par le config, \n",
    "# en plus de l'entrainement du modèle\n",
    "\n",
    "#on incorpore dans la fonction de train un paramètre config qui contient les hyperparamètres\n",
    "def ray_train(config, trainloader, valloader, checkpoint_dir=None, epochs = 10):\n",
    "\n",
    "    #init le reseau et les paramètres d'optimisations\n",
    "    net = MyCNN(config[\"kernel1\"], config[\"kernel2\"])\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "\n",
    "    #choose device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "\n",
    "    #load checkpoint\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    ###### optimization des poids du réseau (boucle d'entraineemnt avec incorporation de tune #########\n",
    "    print(f\"running {net.name}\")\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        net.train()\n",
    "        for x,y in trainloader:                           # boucle sur les batchs  \n",
    "            optimizer.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)                # y doit être un tensor (pas un int)\n",
    "            yhat = net(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            cumloss += l*len(x)                             # attention, il peut y avoir un batch + petit (le dernier)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        \n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        if epoch % 1 == 0:\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for x,y in valloader:\n",
    "                    x,y = x.to(device), y.to(device)\n",
    "                    yhat = net(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(cumloss / count), accuracy = cumacc)\n",
    "    print(\"Finished Training\") \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lancement de ray.tune pour la recherche des hyperparamètres\n",
    "max_num_epochs = 5\n",
    "gpus_per_trial = 0\n",
    "num_samples = 2\n",
    "\n",
    "# on definit les parametres à    optimiser\n",
    "config = {\n",
    "        \"kernel1\" : tune.choice([3, 5, 9]), \n",
    "        \"kernel2\" : tune.choice([3, 5, 9]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        # \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        # \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        # \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "\n",
    "#definit un scheduler qui interrompera les entrainements qui ne progressent pas\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=max_num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2)\n",
    "\n",
    "#definit un reporter qui affiche les resultats\n",
    "reporter = CLIReporter(\n",
    "    # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "\n",
    "\n",
    "#entrainement du réseau en passant par raytune, on peut préciser les ressources utilisées pour chaque entrainement\n",
    "result = tune.run(\n",
    "    partial(ray_train, trainloader= DL_train, valloader= DL_val),\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": gpus_per_trial},\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on récupère le meilleur modèle (avec les meilleurs hyperparamètres)\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "\n",
    "#on récupère la configuration des poids du modèle après entrainements\n",
    "best_trained_model = MyCNN(best_trial.config[\"kernel1\"], best_trial.config[\"kernel2\"])\n",
    "device = \"cpu\"\n",
    "\n",
    "#modif du modele selon les ressources \n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if gpus_per_trial > 1:\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "best_trained_model.to(device)\n",
    "\n",
    "#on récupère les poids du modèle qui l'ont conduit à être le meilleur... cela n'empeche pas de réentrainer après)\n",
    "best_checkpoint_dir = best_trial.checkpoint.value\n",
    "model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "# on teste le modèle sur le jeu de test\n",
    "print('Apres avoir tester le modèle sur jeu de test, accuracy = ...')\n",
    "# test_acc = test_accuracy(best_trained_model, device)\n",
    "# print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 1stConv-{'kernel1': 3, 'kernel2': 3, 'nb_channel1': 8, 'nb_channel2': 8, 'pooling': <class 'torch.nn.modules.pooling.MaxPool2d'>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb Cellule 23\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m'''attention il faut que le nom du fichier contienne toutes les infos sur les hyperparamètres utilisés dans le modèle, \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mpour pouvoir le recharger ensuite (ou bien sauvegarder en plus un fichier de la configuration du modèle utilisé'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m train(model, epochs, DL_train, DL_val) \u001b[39m#ajouter le early stopping !!!!!!!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m DL_test \u001b[39m=\u001b[39m DataLoader(Tdata_val,\u001b[39m1\u001b[39m,shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m acc \u001b[39m=\u001b[39m model_test(model, DL_test)\n",
      "\u001b[1;32m/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb Cellule 23\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, train_loader, test_loader, writer, device, lr)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m x,y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)                \u001b[39m# y doit être un tensor (pas un int)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m yhat \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m l \u001b[39m=\u001b[39m loss(yhat,y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m l\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb Cellule 23\u001b[0m in \u001b[0;36mMyCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time \n",
    "'''search_space dictionnary with all the values of hyperparameters we want to test,\n",
    "on divise en deux les hyperparamètres à tester:\n",
    "- un premier espace de recherche pour l'architecture\n",
    "- un deuxième pour la méthode d'optimisation\n",
    "'''\n",
    "WOT_search_space = {\n",
    "    'lr' : [],\n",
    "    'optimizer' : [],\n",
    "    'batche_size' : [],\n",
    "}\n",
    "\n",
    "\n",
    "arch_search_space = {\n",
    "    'kernel1' : [3,5,9],\n",
    "    'kernel2' : [3,5,9],\n",
    "    'pooling' : [nn.MaxPool2d, nn.AvgPool2d],\n",
    "    'nb_channel1' : [8,16,32], \n",
    "    'nb_channel2' : [8,16,32]\n",
    "}\n",
    "\n",
    "\n",
    "#on fait une recherche sur l'espace de recherche de l'architecture\n",
    "best_accuracy = 0.0\n",
    "for config in ParameterGrid(arch_search_space):\n",
    "    device = 'cpu'\n",
    "    model = MyCNN(kernel1 = config[\"kernel1\"], kernel2= config['kernel2']).to(device)\n",
    "    model.name=\"1stConv-\"+str(config)\n",
    "    '''attention il faut que le nom du fichier contienne toutes les infos sur les hyperparamètres utilisés dans le modèle, \n",
    "    pour pouvoir le recharger ensuite (ou bien sauvegarder en plus un fichier de la configuration du modèle utilisé'''\n",
    "    epochs = 5\n",
    "    train(model, epochs, DL_train, DL_val) #ajouter le early stopping !!!!!!!\n",
    "    DL_test = DataLoader(Tdata_val,1,shuffle=True)\n",
    "    acc = model_test(model, DL_test)\n",
    "    \n",
    "    file = model.name\n",
    "    if acc > 0.95:\n",
    "        save_model(model,'models/' + file)\n",
    "    \n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_model = model\n",
    "        best_config = config\n",
    "        best_file = file\n",
    "        with open('models/info_best_model.txt','w') as f:\n",
    "            f.write(best_file)\n",
    "            f.write(str(best_config))\n",
    "            f.write(str(best_accuracy))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# search = GridSearchCV(model,  arch_search_space, n_jobs=2, verbose=0)\n",
    "# search.fit(x_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e/ Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5501072d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(Tdata_test[0][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Pour utiliser un modèle une fois qu'il est tester... Comment on le fait proprement ? \n",
    "Parce que là j'ai l'impression d'être obliger d'uitliser le dataLoader avec des batch de 1 c'est bizarre'''\n",
    "test = DataLoader(Tdata_val,1,shuffle=True)\n",
    "#on load le modèle\n",
    "device = 'cpu'\n",
    "model = MyCNN().to(device)\n",
    "load_model(\"models/1stConv-Fri Dec 23 01:25:40 2022\", model)\n",
    "model_test(model, test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/ Optimisation d'un SVM\n",
    "\n",
    "Cette section à pour but de mettre en pratique la méthode de GridSearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation packages\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformations des données en x et y\n",
    "def data_transform(data):\n",
    "    y = data['label'].values\n",
    "    x = data.drop('label', axis=1).values\n",
    "    return x,y\n",
    "\n",
    "x_train, y_train = data_transform(data_train)\n",
    "x_test, y_test = data_transform(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''on teste combien il faut d'image pour bien construire un SVM'''\n",
    "list_acc = []\n",
    "list_nb_data = []\n",
    "for i in range (100,1000,50):\n",
    "    xtrain = x_train[:i,:]\n",
    "    nb_data = xtrain.shape[0]\n",
    "    print(nb_data)\n",
    "    ytrain = y_train[:i]\n",
    "    xtest = x_test[:,:]\n",
    "    classif = svm.SVC()\n",
    "    classif.fit(xtrain, ytrain)\n",
    "    ypred = classif.predict(x_test)\n",
    "    list_nb_data.append(nb_data)\n",
    "    list_acc.append(accuracy_score(y_test, ypred))\n",
    "    print(accuracy_score(y_test, ypred))\n",
    "\n",
    "plt.plot(list_nb_data, list_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ayant des ressources de calcul limitées, on ne peut pas tester avec un grand nombre d'image...\n",
    "attention car plus le modèle est grand plus il faut de données pour bien l'entrainer\n",
    "'''\n",
    "x_train = x_train[:,:]\n",
    "y_train = y_train[:]\n",
    "x_test = x_test[:,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a/ Premier SVM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9121428571428571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcafd7b5f10>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classif = svm.SVC()\n",
    "\n",
    "# on fit le model à x_triain et y_train\n",
    "classif.fit(x_train, y_train)\n",
    "\n",
    "# on prédit les valeurs de y_test\n",
    "y_pred = classif.predict(x_test)\n",
    "\n",
    "# on calcule le score de précision multilabel\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "#affichage matrice avec pyplot\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('label')\n",
    "# à rajouter le nombre d'image dans chaque case de la matrice\n",
    "plt.imshow(confusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b/ Optimisation SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(), n_jobs=2,\n",
       "             param_grid=[{'C': [0.1, 0.5, 1, 2, 5],\n",
       "                          'degree': [2, 3, 4, 5, 6, 10], 'kernel': ['poly']},\n",
       "                         {'kernel': ['linear']}])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "### MISE EN PLACE DE LA CHAINE ###\n",
    "classif = svm.SVC()\n",
    "\n",
    "# liste de tuples: titre + objet \n",
    "#pipe = Pipeline([('selvar',selector),('classif',classif)])\n",
    "\n",
    "\n",
    "grid = [\n",
    "    #{\"kernel\": [\"sigmoid\"], \"C\": [0.1, 0.5, 1, 2, 5]},\n",
    "    {\"kernel\": [\"poly\"], \"degree\": [2, 3, 4, 5, 6,10], \"C\": [0.1, 0.5, 1, 2, 5]},\n",
    "    #{\"kernel\": [\"rbf\"],\"gamma\": [0.1, 0.5, 1, 2, 5],  },\n",
    "    {\"kernel\": [\"linear\"]},\n",
    "]\n",
    "\n",
    "# param_grid = list(ParameterGrid(grid))\n",
    "# print(len(param_grid))\n",
    "\n",
    "search = GridSearchCV(classif, grid, n_jobs=2, verbose=0)\n",
    "search.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.904):\n",
      "{'C': 5, 'degree': 2, 'kernel': 'poly'}\n",
      "[0.77  0.692 0.565 0.484 0.418 0.298 0.887 0.837 0.75  0.674 0.583 0.383\n",
      " 0.897 0.864 0.788 0.714 0.632 0.408 0.903 0.878 0.811 0.734 0.669 0.441\n",
      " 0.904 0.887 0.832 0.757 0.693 0.471 0.88 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(search.cv_results_['mean_test_score'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c/ méthode d'ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb Cellule 38\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#on load le meilleur des modèles que l'on ait pu entrainer en faisant notre GridSearch.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1stConv-Fri Dec 23 01:25:40 2022\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m MyCNN()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hippo/Documents/CNN_SVM_Mnist/HD_analysis.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m load_model(\u001b[39m'\u001b[39m\u001b[39mmodels/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m file, model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyCNN' is not defined"
     ]
    }
   ],
   "source": [
    "#on load le meilleur des modèles que l'on ait pu entrainer en faisant notre GridSearch.\n",
    "file = '1stConv-Fri Dec 23 01:25:40 2022'\n",
    "model = MyCNN()\n",
    "load_model('models/' + file, model) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/ GAN model generating digit "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the others notebooks on my Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on charge un GAN prentrainé de pytorch\n",
    "from torchvision import models\n",
    "model = models.dcgan.dcgan_generator(100, 3, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on affiche les variables de l'environnement\n",
    "%who"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baacd7ded0742aa8408bda3ed6ced71320ba4869ece4e05e453d0cf31ed1376f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
